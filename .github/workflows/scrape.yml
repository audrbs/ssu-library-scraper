# 워크플로우 이름
name: Scrape Library Seats

# 이 워크플로우를 언제 실행할 것인지
on:
  # 1. 30분마다 자동으로 실행
  schedule:
    - cron: '*/15 * * * *'  # 30분 간격 (15분은 '*/15')
  # 2. GitHub 페이지에서 수동으로 실행할 수도 있게 함
  workflow_dispatch:

# 쓰기 권한 추가  
permissions:
  contents: write
  
# 실행할 작업(Job) 정의
jobs:
  scrape-data:
    # 1. 작업 환경 (최신 우분투 리눅스)
    runs-on: ubuntu-latest

    # 2. 실제 실행 단계 (Steps)
    steps:
      # (1) 저장소에 있는 코드를 가상 서버로 복사
      - name: 1. Checkout repository
        uses: actions/checkout@v3

      # (2) 파이썬 3.10 버전 설치
      - name: 2. Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # (3) requirements.txt를 이용해 라이브러리 설치
      - name: 3. Install dependencies
        run: pip install -r requirements.txt

      # (4) 우리가 만든 파이썬 스크립트 실행
      - name: 4. Run scraper script
        run: python scraper.py

      # (5) 스크립트 실행으로 변경된 CSV 파일을 GitHub에 다시 저장(커밋)
      - name: 5. Commit and push data
        run: |
          git config --global user.name 'github-actions'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add recliner_seats_log.csv  # 수집된 데이터 파일만 추가
          # 변경 사항이 있을 때만 커밋 (오류 방지)
          git diff --staged --quiet || git commit -m "Auto-update recliner seat data"
          git push
